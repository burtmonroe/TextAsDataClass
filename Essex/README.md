# Course schedule - Essex 2P Advanced Text as Data / NLP

### Day 1 (Jul 26) - Introduction and Overview

Kenneth Benoit. 2020. “Text as Data: An Overview.” In Robert Franzese and Luigi Curini, eds. SAGE Handbook of Research Methods in Political Science and International Relations. [here](https://kenbenoit.net/pdfs/CURINI_FRANZESE_Ch26.pdf)

Jacob Eisenstein. 2018. “Introduction.” Natural Language Processing [here](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)

Notes: [Open Source Tools for Text as Data / NLP in R](https://burtmonroe.github.io/TextAsDataCourse/Notes/RText/)

Notes: [Open Source Tools for Text as Data / NLP in Python](https://burtmonroe.github.io/TextAsDataCourse/Notes/PythonText/)

Slides: [https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day1-Introduction](https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day1-Introduction)

R Notebooks for Day 1 [https://rstudio.cloud/](https://rstudio.cloud/).

Python notebook on string manipulation and regular expressions: [here](https://colab.research.google.com/drive/1wCVf8xaoTAsKya5uuuo5knvizbWgheE_?usp=sharing)


### Day 2 (Jul 27) - Language Models and NLP Pipelines for Sequence Labeling

Dan Jurafsky & James Martin (2020), Speech & Language Processing (3rd edition draft). Chapters 3, 8, 14. “N-gram Language Models,” “Sequence Labeling for Parts of Speech and Named Entities,” “Dependency Parsing.” [here](https://web.stanford.edu/~jurafsky/slp3/)

Slides: [https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day2-NLPPipelines.pdf](https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day2-NLPPipelines.pdf)

R Tutorials for Day 2 on RStudio Cloud: NLP Pipelines in R + spaCy in R (RStudio Cloud)

Python Tutorials on NLP Annotation Pipelines (spaCy and Stanza for now) [here](https://colab.research.google.com/drive/1Us7Hx5xF5pdx-JM3t_6QB8SZZhHfrc0Q?usp=sharing)


### Day 3 (Jul 28) - Word Embeddings

Dan Jurafsky & James Martin (2020), Speech & Language Processing (3rd edition draft). Chapter 6, “Vector Semantics and Embeddings.” [here](https://web.stanford.edu/~jurafsky/slp3/)

Pedro Rodriguez and Arthur Spirling (Forthcoming) “Word embeddings: What works, what doesn’t, and how to tell the difference for applied research.” Journal of Politics. [here](https://github.com/ArthurSpirling/EmbeddingsPaper/blob/master/Paper/Embeddings_SpirlingRodriguez.pdf)

Slides: [https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day3-WordEmbeddings.pdf](https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day3-WordEmbeddings.pdf)

Python Tutorial on Estimating Word Embeddings with gensim: https://colab.research.google.com/drive/1eSzd2z5B3CDeTxpdMXCIh3bm1L-gYzCr?usp=sharing


### Day 4-5 (Jul 29-30) - Neural Networks and Deep Learning for NLP

Dan Jurafsky & James Martin (2020), Speech & Language Processing (3rd edition draft). Chapter 7, “Neural Networks and Neural Language Models.” [here](https://web.stanford.edu/~jurafsky/slp3/)

Jay Alammar. 2016. “A Visual and Interactive Guide to the Basics of Neural Networks.” [here](https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/)

Jay Alammar. 2016. “A Visual and Interactive Look at Basic Neural Network Math.” [here](https://jalammar.github.io/feedforward-neural-networks-visual-interactive/)

Christopher Olah. 2014. “Deep Learning, NLP, and Representations.” [here](http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)

Kakia Chatsiou and Slava Jankin Mikhaylov. 2020. “Deep Learning for Political Science.” In Robert Franzese and Luigi Curini, eds. SAGE Handbook of Research Methods in Political Science and International Relations. [here](https://arxiv.org/pdf/2005.06540.pdf)

Slides: [https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day4-DeepLearningNLP.pdf](https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day4-DeepLearningNLP.pdf)

R Tutorial on Text Classification with Keras and Tensorflow, on RStudio Cloud in Day 4 Project

Python Tutorial on Text Classification with Keras and Tensorflow: https://colab.research.google.com/drive/1MG2_5Hx5dwN77hmVNY0aUiGo99k2mPGb?usp=sharing

### Aug 2

Study guide for exams: https://docs.google.com/document/d/1eZGUUzqTJCfQjQiJAn1DqhaVWeDoH4RSvOU8TC3WIqE/edit?usp=sharing

Slides: [https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day5-DeepLearningNLP2.pdf](https://burtmonroe.github.io/TextAsDataCourse/Essex/EssexNLP-Day5-DeepLearningNLP2.pdf)

Text Classification with Keras and Tensorflow 2: Dropout and Weight Regularization (Python): https://colab.research.google.com/drive/1kGhXArEbWDP_A4TtlB1cgSubekIsX4VP?usp=sharing

Text Classification with Keras and Tensorflow 2: Dropout and Weight Regularization (R): https://colab.research.google.com/drive/1hq9eCrWjDOkpMUY0QJ9fAOHWagcBSXU7?usp=sharing

Text Classification with Keras and Tensorflow 3: Pretrained Embeddings (Python): https://colab.research.google.com/drive/1pkJNzWDdqTaVzZFQ1RnkAxx87Wkyr31T?usp=sharing

Text Classification with Keras and Tensorflow 3: Pretrained Embeddings (R): Coming shortly.

Text Classification with Keras and Tensorflow 4: Incorporating an Embedding Layer (Python): https://colab.research.google.com/drive/1_6m2DVFQJPZH5UENZDs7jkrOU6kjyuCu?usp=sharing

Text Classification with Keras and Tensorflow 4: Incorporating an Embedding Layer (R): Coming shortly.


### Aug 3 - From Recurrent Neural Networks to Transformers

Dan Jurafsky & James Martin (2020), Speech & Language Processing (3rd edition draft). Chapter 9, “Deep Learning Architectures for Sequence Processing.” [here](https://web.stanford.edu/~jurafsky/slp3/)

Jay Alammar. 2018. "The Illustrated Transformer." [here](https://jalammar.github.io/illustrated-transformer/)

Suggested: Andrew Halterman. 2019. “Geolocating Political Events.” [here](https://arxiv.org/pdf/1905.12713.pdf)

Suggested: Han Zhang and Jennifer Pan. 2019. “CASM: A Deep-Learning Approach for Identifying Collective Action Events from Text and Image Data.” Sociological Methodology 49(1): 1-57. [here](http://jenpan.com/jen_pan/casm.pdf)



### Aug 4 - Contextual Embeddings, Pretrained Language Models, and Transfer Learning

Noah Smith. 2019. “Contextual Word Vectors: A Contextual Introduction.”

Jay Alammar. 2018. “The Illustrated BERT, ELMo and Co. (How NLP Cracked Transfer Learning).” [here](http://jalammar.github.io/illustrated-bert/)

Jay Alammar. 2019. “A Visual Guide to Using BERT for the First Time.” [here](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)

Zhanna Terechskenko, Fridolin Linder, Vishakh Padmakumar, Michael Liu, Jonathan Nagler, Joshua A. Tucker, and Richard Bonneau. 2020. “A Comparison of Methods in Political Science Text Classification: Transfer Learning Language Models for Politics.” [here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3724644)


### Aug 5 - Multilingual Text as Data and Machine Translation

Mitchell Goist and Burt L. Monroe. 2020. “Scaling the Tower of Babel: Common-Space Analysis of Political Text in Multiple Languages.”

Leah C. Windsor, James G. Cupit, Alistair J. Windsor. 2019. “Automated content analysis across six languages.” PloS ONE 14(11):e0224425. [here](https://doi.org/10.1371/journal.pone.0224425)


### Aug 6 - Natural Language Understanding and Natural Language Generation / Fairness & Bias in NLP

TBA. May include 

Jay Alammar. 2019. “The Illustrated GPT-2 (Visualizing Transformer Language Models)” [here]( http://jalammar.github.io/illustrated-gpt2/)

Jay Alammar. 2020. "How GPT-3 Works: Visualizations and Animations." [here](https://jalammar.github.io/how-gpt3-works-visualizations-animations/)

