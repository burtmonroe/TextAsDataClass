# Course schedule - Essex 2P Advanced Text as Data / NLP

### Day 1 (Jul 26) - Introduction and Overview

Kenneth Benoit. 2020. “Text as Data: An Overview.” In Robert Franzese and Luigi Curini, eds. SAGE Handbook of Research Methods in Political Science and International Relations. https://kenbenoit.net/pdfs/CURINI_FRANZESE_Ch26.pdf

Jacob Eisenstein. 2018. “Introduction.” Natural Language Processing https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf

Notes: [Open Source Tools for Text as Data / NLP in R](https://burtmonroe.github.io/TextAsDataCourse/Notes/RText/)

Notes: [Open Source Tools for Text as Data / NLP in Python](https://burtmonroe.github.io/TextAsDataCourse/Notes/PythonText/)


### Day 2 (Jul 27) - Language Models and NLP Pipelines for Sequence Labeling

Dan Jurafsky & James Martin (2020), Speech & Language Processing (3rd edition draft). Chapters 3, 8, 14. “N-gram Language Models,” “Sequence Labeling for Parts of Speech and Named Entities,” “Dependency Parsing.” https://web.stanford.edu/~jurafsky/slp3/

R Tutorials: NLP Pipelines in R + spaCy in R (RStudio Cloud)

Python Tutorials: Intro to spaCy + Intro to Stanza (Google Colab)


### Day 3 (Jul 28) - Word Embeddings

Dan Jurafsky & James Martin (2020), Speech & Language Processing (3rd edition draft). Chapter 6, “Vector Semantics and Embeddings.” https://web.stanford.edu/~jurafsky/slp3/

Pedro Rodriguez and Arthur Spirling (Forthcoming) “Word embeddings: What works, what doesn’t, and how to tell the difference for applied research.” Journal of Politics. https://github.com/ArthurSpirling/EmbeddingsPaper/blob/master/Paper/Embeddings_SpirlingRodriguez.pdf



### Day 4-5 (Jul 29-30) - Neural Networks and Deep Learning for NLP

Dan Jurafsky & James Martin (2020), Speech & Language Processing (3rd edition draft). Chapter 7, “Neural Networks and Neural Language Models.” https://web.stanford.edu/~jurafsky/slp3/

Jay Alammar. 2016. “A Visual and Interactive Guide to the Basics of Neural Networks.” https://jalammar.github.io/visual-interactive-guide-basics-neural-networks/

Jay Alammar. 2016. “A Visual and Interactive Look at Basic Neural Network Math.” https://jalammar.github.io/feedforward-neural-networks-visual-interactive/

Christopher Olah. 2014. “Deep Learning, NLP, and Representations.” http://colah.github.io/posts2014-07-NLP-RNNs-Representations/

Kakia Chatsiou and Slava Jankin Mikhaylov. 2020. “Deep Learning for Political Science.” In Robert Franzese and Luigi Curini, eds. SAGE Handbook of Research Methods in Political Science and International Relations. https://arxiv.org/pdf/2005.06540.pdf


### Day 6 (Aug 2) - From Recurrent Neural Networks to Transformers

Dan Jurafsky & James Martin (2020), Speech & Language Processing (3rd edition draft). Chapter 9, “Deep Learning Architectures for Sequence Processing.” https://web.stanford.edu/~jurafsky/slp3/

Jay Alammar. 2018. "The Illustrated Transformer." https://jalammar.github.io/illustrated-transformer/

Suggested: Andrew Halterman. 2019. “Geolocating Political Events.” https://arxiv.org/pdf/1905.12713.pdf

Suggested: Han Zhang and Jennifer Pan. 2019. “CASM: A Deep-Learning Approach for Identifying Collective Action Events from Text and Image Data.” Sociological Methodology 49(1): 1-57. http://jenpan.com/jen_pan/casm.pdf



### Day 7 (Aug 3) - Contextual Embeddings, Pretrained Language Models, and Transfer Learning

Noah Smith. 2019. “Contextual Word Vectors: A Contextual Introduction.”

Jay Alammar. 2018. “The Illustrated BERT, ELMo and Co. (How NLP Cracked Transfer Learning).” http://jalammar.github.io/illustrated-bert/

Jay Alammar. 2019. “A Visual Guide to Using BERT for the First Time.” http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/

Zhanna Terechskenko, Fridolin Linder, Vishakh Padmakumar, Michael Liu, Jonathan Nagler, Joshua A. Tucker, and Richard Bonneau. 2020. “A Comparison of Methods in Political Science Text Classification: Transfer Learning Language Models for Politics.” https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3724644


### Day 8 (Aug 4) - Multilingual Text as Data and Machine Translation

Mitchell Goist and Burt L. Monroe. 2020. “Scaling the Tower of Babel: Common-Space Analysis of Political Text in Multiple Languages.”

Leah C. Windsor, James G. Cupit, Alistair J. Windsor. 2019. “Automated content analysis across six languages.” PloS ONE 14(11):e0224425. https://doi.org/10.1371/journal.pone.0224425


### Day 9 (Aug 5) - Natural Language Understanding and Natural Language Generation

TBA. May include 

Jay Alammar. 2019. “The Illustrated GPT-2 (Visualizing Transformer Language Models)” http://jalammar.github.io/illustrated-gpt2/

Jay Alammar. 2020. "How GPT-3 Works: Visualizations and Animations." https://jalammar.github.io/how-gpt3-works-visualizations-animations/


### Day 10 (Aug 6) - Fairness and Bias in NLP

TBA.
